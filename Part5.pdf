
After talkiing to the doctors in my family and showing them these results I could grasp the interpretation much better and this is what I realised-


1. Most Dangerous Failure Mode
Honestly, I think it is not a crash or a wrong prediction it is the system confidently saying normal while a patient is slowly getting worse.
The scenario I kept coming back to while building this: a patient's HR climbs from 78 to 118 over six minutes, SpO₂ drops from 97% to 93%. Neither value, at any single moment, looks alarming enough. The slope feature only sees the last 30 seconds of a six-minute trend. The LSTM reconstruction error nudges up slightly but not enough to cross the threshold. The ensemble says normal. The paramedic sees nothing. By the time the patient looks visibly distressed, the window for early intervention is gone.
What makes this the scariest failure is that nothing is broken. The models are doing exactly what they were trained to do. There is no bug to point at. The gap is in the design for now ,short windows are structurally blind to slow deterioration and I only fully understood that after building the whole thing and staring at the results.

2. Reducing False Alerts Without Missing Deterioration
This was the hardest part of the project and I do not think there is a perfect answer, only trade-offs you have to be deliberate about.
What I ended up doing was layering three things rather than relying on one. First, the ensemble ,three models with different approaches means a random motion spike in the Isolation Forest score usually does not move the weighted average enough on its own. Second, clinical context in the severity multipliers like a borderline SpO₂ of 93% alone does not trigger a critical alert, it needs co-occurring tachycardia or a sustained trend to get there. A single mildly abnormal reading almost never means an emergency, and the scoring reflects that. Third, the persistence filter ,requiring three consecutive anomalous windows before alerting killed most of the one-off artefact spikes. The cost is 15 seconds of extra latency for non-critical events, which felt like a fair trade.
The one thing I was not willing to move on was recall. Every time I tightened a threshold to reduce false alerts, I checked that the real distress scenarios — tachycardia plus hypoxaemia, rapid HR slope — were still being caught first. False alarms are frustrating. Missed emergencies are a different category of problem entirely.

3. What Should Never Be Fully Automated
The decision to actually intervene.
The system can tell a paramedic something looks wrong. It can explain why, show which vitals are driving the alert, say how long the pattern has been building, and give a confidence score. That is genuinely useful. What it cannot do is know that this particular patient is 74 years old, is on beta-blockers that suppress their HR response, and had a similar episode last year that turned out to be nothing. The model does not have that. The person in the ambulance does.
There is also a simpler reason. When something goes wrong in a medical setting , and eventually something always does ,there has to be a clear answer to the question: who decided this? The moment a system acts on a patient autonomously, that accountability gets blurry in a way that is dangerous not just ethically but practically. You cannot debrief an algorithm.
What I would fully automate is everything upstream that is the monitoring, the pattern detection, the alert, getting the right information to the right person fast. What I would never automate is the moment of clinical judgement. The whole point of this system, as I see it, is to make that moment better informed and faster and not to skip it.